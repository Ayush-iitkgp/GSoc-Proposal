{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contact\n",
    "\n",
    "**Name**            :   Ayush Pandey\n",
    "\n",
    "**University**      :   [Indian Institute of Technology (IIT), Kharagpur](http://iitkgp.ac.in)\n",
    "\n",
    "**Email**           :   ayushpandey.iitkgp@gmail.com\n",
    "\n",
    "**IRC Handle**      :   KrishnaKanhaiya at freenode.net\n",
    "\n",
    "**Github Username** :   [Ayush-iitkgp](https://github.com/Ayush-iitkgp)\n",
    "\n",
    "**Mentor**          :   [Christopher Rackauckas](https://github.com/ChrisRackauckas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Project\n",
    "\n",
    "## The Problem and Motivations\n",
    "The aim of the project is to add different optimization and bayesian techniques to estimate the parameters of the differential equation in DiffEqParamEstim.jl package and do a comprehensive study of the pros and cons of the different algorithms to add it to the documentation.\n",
    "\n",
    "Differential equation models are widely used in many scientific fields that include engineering, physics, and biomedical sciences. The so-called “forward problem” that is the problem of solving differential equations for given parameter values in the differential equation models has been extensively studied by mathematicians, physicists, and engineers. \n",
    "\n",
    "However, the **inverse problem**, the problem of parameter estimation based on the measurements of output variables, has not been well explored using modern optimization and statistical methods. Parameter estimation aims to find the unknown parameters of the model which give the best fit to a set of experimental data. In this way, parameters which cannot be measured directly will be determined in order to ensure the best fit of the model with the experimental results. This will be done by globally minimizing an objective function which measures the quality of the fit. This inverse problem usually considers a cost function to be optimized (such as maximum likelihood). This problem has applications in systems biology, HIV-AIDS study, and drug dosage estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Plan\n",
    "I propose to implement different algorithms for estimation of the parameters of the differential equations such as:\n",
    "\n",
    "1. Implement support for more Kernels for two-stage method\n",
    "2. Implement support for different distance measures such as (log-)likelihood and expectation maximization algorithm\n",
    "3. Implement regularization techniques such as Tikhonov regularization, LASSO regularization and integrate with PenaltyFunctions.jl\n",
    "4. Provide support for deterministic and heuristic algorithms for global optimum such as BlackBoxOptim, genetic algorithm\n",
    "5. Implement model based smoothing approach to estimate the parameters\n",
    "6. Bayesian techniques for parameter estimation using Stan.jl\n",
    "\n",
    "**Stretch Goals**\n",
    "\n",
    "1. Implementing support for parameter estimation for stochastic differential equations   \n",
    "2. Writing native Julia bayesian computational engine using Mamba.jl and Klara.jl\n",
    "\n",
    "\n",
    "I also aim to do a detailed study of the methods and write a proper documentation on where to use the algorithms, precautions to take while using them and the pros and cons of the above methods.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "First of all, we need to understand the mathematical formulation of the inverse parameter estimation problem before we get into the implementation details.\n",
    "\n",
    "### General Form of Parameter Estimation Problem\n",
    "\n",
    "![](images/General_Parameter_Estimation_Form.png)\n",
    "\n",
    "where $x \\in  \\mathbf{R}^{n}$ is the observed state vector, $\\theta \\in {R}^{m}$ is the vector of unknown parameters, f(.) is a known linear or non-linear function vector. In general, we may not observe x directly but we can observe a function of x, here g(.) is the observation functions that maps the state variable to a vector of observable quantities $y \\in {R}^{n}$, these are the signals that can be measured in the experiments.\n",
    "\n",
    "\n",
    "### Maximum likelihood and cost function\n",
    "\n",
    "Maximum likelihood and cost function Assuming that the transformed measurements y are contaminated by additive normally distributed uncorrelated random measurement errors i.e. $y_{ijk}$ = $y_{ijk}(x(t_{i}), θ) + e_{ijk}$ where $e_{ijk}$ ∼ N(0, $σ2_{ijk})$ is the random error with standard deviation $σ_{ijk}$ and $y_{ijk}$ is the measured value, the estimation of the model parameters is formulated as the maximization of the likelihood of the data:\n",
    "\n",
    "![](images/Maximum_Likelihood.png)\n",
    "\n",
    "where $N_{e}$ is the number of experiments, $N_{y,k}$ is the number of observed compounds in the kth experiment, and $N_{t,k,j}$ is the number of measurement time points of the jth observed quantity in the kth experiment.\n",
    "\n",
    "From the theory of maximum likelihood, **the objective is to maximize the likelihood function** and estimate the parameters for which the likelihood function is maximized.\n",
    "\n",
    "Since the likelihood function consists of product terms, we take the log of the likelihood function and negate it to ease our calculation, thus we get log-likelihood function as follows:\n",
    "\n",
    "![](images/Maximum_log_Likelihood.png)\n",
    "\n",
    "**Note** - The maximization of the likelihood function (4) is equivalent to the minimization of the weighted least squares cost function (5).\n",
    "\n",
    "where the residual vector R(·) : $R^{Nθ}$ → $R^{ND}$ is constructed from the squared terms by arranging them to a vector. With this, the model calibration problem can be stated as the well-known nonlinear least-squares (NLS) optimization problem:\n",
    "\n",
    "![](images/NLS.png)\n",
    "\n",
    "A θ vector that solves this optimization problem is called the maximum-likelihood estimates of model parameters.\n",
    "\n",
    "Therefore, the objective of the \"**inverse problem**\" is to pose the problem as the optimization problem and get the optimum value to estimate the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution\n",
    "\n",
    "## 1. Implement support for more Kernels for two-stage method\n",
    "\n",
    "This method is inspired by the research paper [Parameter estimation for Differential Equation Models using a Framework of Measurement Error in Regression Models](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2631937/) where the idea is to estimate the parameters of the Ordinary Differential Equations (ODE) using local smoothing approach and a pseudo-least square.\n",
    "\n",
    "**Note** - This method only works for ordinary differential equations models.\n",
    "\n",
    "Let's rewrite the differential equation written above in another form:\n",
    "\n",
    "![](images/2_stage_form_ODE.png)\n",
    "\n",
    "where X(t) = ${ X1(t), …, Xk(t)}^{T}$ is an unobserved state vector, β = $(β1, …, βm)^{T}$ is a vector of unknown parameters, and F(·) = ${F1(·), …, Fk(·)}^{T}$ is a known linear or nonlinear function vector. In practice, we may not observe X(t) directly, but we can observe its surrogate Y(t). For simplicity, assume an additive measurement error model to relate X(t) to the surrogate Y(t), i.e.,\n",
    "\n",
    "![](images/2_stage_form_Y.png)\n",
    "\n",
    "where the measurement error e(t) is independent of X(t) with a covariance matrix $Σ_{e}$.\n",
    "\n",
    "Suppose X̂′(t) is an estimator of X′(t). Substituting the estimates X̂′(ti), i = 1, …, n, in the ODE equation above, we obtain a regression model:\n",
    "\n",
    "![](images/2_stage_form_reg.png)\n",
    "\n",
    "where Δ($t_{i}$) denotes the substitution error vector, that is Δ($t_{i}$) = X̂′($t_{i}$)− X′($t_{i}$). If X′($t_{i}$) is an unbiased estimator of X′($t_{i}$), Δ($t_{i}$) are errors with mean zero but are not independent. However, if the estimator X′($t_{i}$) is a biased estimator.\n",
    "\n",
    "The idea is to estimate X(ti) and X′(ti) using regression techniques such as local polynomial regression, smoothing spline and the regression spline.\n",
    "\n",
    "I have already [implemented](https://github.com/JuliaDiffEq/DiffEqParamEstim.jl/pull/6), a two-stage method which uses local linear regression to estimate X(t) and local quadratic regression to estimate X′(t). \n",
    "\n",
    "As a consequence, the estimators X̂(t) and X̂′(t) can be expressed as \n",
    "\n",
    "![](images/Parameter_Estimation.png)\n",
    "\n",
    "where \n",
    "![](images/2_stage_form_reg_W.png)\n",
    "\n",
    "and K(.) is a symmetric kernel function $K_{h}$(.) = K(./h)/h and h is a proper bandwidth.\n",
    "\n",
    "Currently, the implementation supports **Epanechnikov**, **Uniform** and **Triangular** kernel functions. I propose to include more kernels such as Quartic, Triweight, Tricube, Gaussian, Cosine, Logistic, Sigmoid function, and Silverman.\n",
    "\n",
    "I also plan to include the advantages and the disadvantages of this method such as :\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "1. Computational efficiency\n",
    "2. Easing of the convergence problem\n",
    "3. The initial values of the state variables of the differential equations not required\n",
    "4. Providing good initial estimates of the unknown parameters for other computationally-intensive methods to further refine the estimates rapidly\n",
    "\n",
    "**Disadvantage**\n",
    "\n",
    "1. This method does not converge to the global/local minima as the Non-Linear regression does in the objective function is convex/concave.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement support for different distance measures such as (log-)likelihood and expectation \n",
    "In the present implementation the \"build_loss_objective\" function only covers the Least-Square objective function and the function defined in [LossFunctions.jl](https://github.com/JuliaML/LossFunctions.jl). It also assumes that the errors in measurement are not correlated i.e\n",
    "\n",
    "![](images/Maximum_Likelihood_no_covariance.png)\n",
    "\n",
    "Then the maximum likelihood estimate of x given observations y is the solution to the non-linear least squares problem:\n",
    "\n",
    "![](images/Maximum_Likelihood_no_covariance_optimal.png)\n",
    "\n",
    "This type of objective function can easily be constructed using LossFunction.jl. However, there are cases when the error is correlated such as:\n",
    "\n",
    "![](images/Maximum_Likelihood_covariance_.png)\n",
    "\n",
    "then the maximum likelihood problem to be solved is:\n",
    "\n",
    "![](images/Maximum_Likelihood_covariance_optimal.png)\n",
    "\n",
    "Thus, I plan to include the functionality to provide the user with te option to state whether the errors are correlated or not and accordingly build the cost/objective function.Also, the present implementation assumes that the variance of the error terms is same which may always be not the case. I will also implement an option for the **weighted least square cost function**\n",
    "\n",
    "The log-likelihood function reduces to weighted least squares when the error is normally distributed which may not be the case always. Thus, the idea way to go about it would be to provide the option to construct the likelihood function as the cost function. Log-likelihood should also support different versions depending on the type of experimental noise:\n",
    "1. 'homo' homoscedastic noise with constant variance               \n",
    "2. 'homo_var' homoscedastic noise with known non-constant variance\n",
    "3. 'hetero' heteroscedastic noise with variance proportional to the observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement regularization techniques such as Tikhonov regularization, LASSO regularization and integrate with PenaltyFunctions.jl\n",
    "\n",
    "Regularization aims to make the problem less complex (more regular), i.e. to ensure the uniqueness of the solution, to reduce the ill-conditioning and to avoid model overfitting. These techniques are ways to surmount ill-posedness and ill-conditioning.\n",
    "\n",
    "For non-linear cost function, there is no general recipe for the selection of the regularization method. The idea is to provide the users with the options to choose from. [PenaltyFunctions.jl](https://github.com/JuliaML/PenaltyFunctions.jl) provides two main families of penalty functions namely **Element Penalties** and **Array Penalties** which can be used as the regularization function.\n",
    "\n",
    "A comprehensive list of penalty functions provided by PenaltyFunctions.jl are:\n",
    "\n",
    "#### Element Penalties \n",
    "\n",
    "\n",
    "Penalty       | value on element\n",
    "--------------|-----------------\n",
    "`NoPenalty()` | `g(θ) = 0`\n",
    "`L1Penalty()` | `g(θ) = abs(θ)`\n",
    "`L2Penalty()` | `g(θ) = 0.5 * θ ^ 2`\n",
    "`ElasticNetPenalty(α = 0.5)` | `g(θ) = (1 - α) * abs(θ) + α * .5 * θ ^ 2`\n",
    "`SCADPenalty(a = 3.7, γ = 1.0)` | `L1Penalty that blends to constant`\n",
    "`MCPPenalty(γ = 2.0)` | `g(θ) = abs(θ) < γ ? abs(θ) - θ ^ 2 / 2γ : γ / 2`\n",
    "`LogPenalty(η = 1.0)` | `g(θ) = log(1 + η * abs(θ))`\n",
    "\n",
    "\n",
    "\n",
    "#### Array Penalties\n",
    "\n",
    "Penalty                | value on array\n",
    "-----------------------|-----------------\n",
    "`NuclearNormPenalty()` | `sum of singular values of x`\n",
    "`MahalanobisPenalty(C)`| `g(x) = x' * C' * C * x`\n",
    "`GroupLassoPenalty()`  | `g(x) = vecnorm(x)`\n",
    "\n",
    "\n",
    "I propose to integrate the PenaltyFunctions.jl with DiffeEqParamEstim to support the different penalty functions as regularization terms in the cost function.\n",
    "\n",
    "There are 2 other penalty type regularization techniques which should be supported, these are:\n",
    "\n",
    "#### Tikhonov regularization\n",
    "\n",
    "\n",
    "![](images/regularization_tikhonov.png)\n",
    "\n",
    "where W ∈ $R^{Nθ×Nθ}$ is a diagonal scaling matrix and $θ^{ref}$ ∈ $R^{Nθ}$ is a reference parameter vector.\n",
    "\n",
    "#### LASSO regularization\n",
    "\n",
    "∑$θ_{i}$ <= t\n",
    " \n",
    "where t is the value provided by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Provide support for deterministic and heuristic algorithms for global optimum such as BlackBoxOptim, genetic algorithm\n",
    "\n",
    "It is well-known that the cost function (5) can be highly nonlinear and nonconvex in the model parameters. Many efficient local optimization algorithms have been developed to find the solution of nonlinear least squares problems, including Gauss-Newton, Levenberg-Marquardt and trust-region methods. These local methods are especially efficient when provided with high quality first (gradient, Jacobian) and second order (Hessian) information via parametric sensitivities. However, in this type of problems they will likely converge to local solutions close to the initial guess of the parameters. Thus, there is a need to integrate global optimization algorithm packages with DiffEqParamEstim. [JuliaOpt]() currently supports the following global optimization algorithms:\n",
    "1. [BlackBoxOptim](https://github.com/robertfeldt/BlackBoxOptim.jl)\n",
    "2. [Evolutionary](https://github.com/wildart/Evolutionary.jl)\n",
    "3. [GeneticAlgorithms](https://github.com/WestleyArgentum/GeneticAlgorithms.jl)\n",
    "4. [StochasticSearch](https://github.com/phrb/StochasticSearch.jl) such as Tabu Search and Simulated Annealing\n",
    "\n",
    "The idea is to mould the cost function (least square or likelihood) generated during parameter estimation to be in the format accepted by the above solvers.\n",
    "\n",
    "**Note** - Deterministic global optimization methods can guarantee global optimality but their computationally cost increases exponentially with the number of estimated parameters. Alternatively, stochastic and heuristic methods can be used as more practical alternatives, usually obtaining adequate solutions in reasonable computation times, although at the price of no guarantees. In such context, metaheuristics, hybrids (i.e. combinations) with efficient local search methods have been particularly successful, I would also like to look at an approach to combine the deterministic and the heuristic techniques to converge to the exact global optimum in a reasonable time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Implement model based smoothing approach to estimate the parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bayesian techniques for parameter estimation using Stan.jl\n",
    "\n",
    "<!-- **Note- ** The **\"evaluate\"** method in the existing atoms can handle the complex variables. But some atoms are only defined over real domain (for example- max(x,0)) thus, I would need to append the warning messages to such atoms in case the users by mistake call the evaluate method with complex argument. --->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Me\n",
    "\n",
    "## Personal Background\n",
    "\n",
    "I am a final year graduate student pursuing an Integrated Master of Science(MS) degree in Mathematics and Computing Sciences with specialization in Optimization at IIT Kharagpur, India. I am proficient in C, C++, Python, and Julia.\n",
    "\n",
    "## Previous Relevant Experience\n",
    "\n",
    "I am a former Google Summer of the Code student with the Julia Language where I worked on the project titled **Support for complex numbers within Convex.jl**. As a result of our work, we became the first open-source DCP package to support optimization with complex-variables. I have also worked on different projects related to optimization and machine learning. Please find all my projects [here](https://drive.google.com/drive/folders/0B2oOdWdSJWa1RWRsQWdCZ25LUXc). I have taken courses on differential equations such as Partial Differential Equations, Numerical solution of ODE and PDE and Advanced NUmnerical Techniques(Theory and Lab) where I have written MATLAB/C [code](https://github.com/Ayush-iitkgp/Numerical-Solution-of-ODE-and-PDE) for different numerical techniques used in solving ODEs and PDEs. \n",
    "\n",
    "## Relevant Courses\n",
    "* Partial Differential Equations\n",
    "* Numerical solution of ODE and PDE\n",
    "* Advanced Numerical Techniques\n",
    "* Stochastic Processes\n",
    "* Non-Linear Programming\n",
    "* Convex Optimization\n",
    "* Probability & Statistics, Regression, Generalized Linear Models\n",
    "* Linear Algebra, Programming and Data Structures, Object Oriented System Design\n",
    "\n",
    "\n",
    "## Answers of listed questions\n",
    "\n",
    "`1. What do you want to have completed by the end of the program?`\n",
    "\n",
    "By the end of the program, I want to support different optimization and bayesian techniques used to estimate the parameters of the differential equations given the experimental data. Presently, DiffEqParamEstim only supports Non-Linear Regression technique to estimate the parameters. I aim to implement numerous techniques such as two-stage method, model relaxation technique, regularization, log-likelihood estimation, meta-heuristic techniques for global optimum and bayesian techniques using Stan.jl.\n",
    "\n",
    "`2. Who’s interested in the work, and how will it benefit them?`\n",
    "\n",
    "The problem of parameter estimation has extensive use in the following fields:\n",
    "1. HIV-AIDS viral dynamics\n",
    "2. Systems biology\n",
    "3. Drug dosage estimation\n",
    "\n",
    "`3. What are the potential hurdles you might encounter, and how can you resolve them?`\n",
    "\n",
    "In order to link Stan with DiffEqParamEstim, I will have to understand how Stan works. I will also need to go through the theory of Markov Chain Monte Carlo and Hamiltonian Monte Carlo to write in-situ Bayesian estimator for parameters of the diffeenetial equations. Lastly, I would need to understand the theoty behind the stochastic differential equations to write an implementation to find it's parameters. \n",
    "\n",
    "`4. How will you prioritise different aspects of the project like features, API usability, documentation and robustness?`\n",
    "\n",
    "Please refer to the \"Timeline\" section where I have described in details about my plans to tackle different aspects of the project.\n",
    "\n",
    "`5. Does your project have any milestones that you can target throughout the period?`\n",
    "\n",
    "Yes, before JuliaCon 2017, I would have implemented the various optimization and regualarization algorithms to estimate the parameters and would be ready for testing for the Julia community.\n",
    "\n",
    "`6. Are there any stretch goals you can make if the main project goes smoothly?`\n",
    "\n",
    "Yes, I would try to implement native julia computational engine for Bayesian estimation using tools like Mamba.jl or Klara.jl and the inverse problem for Stochastic Differential Equations based on the paper \"[Control of Stochastic and Induced Switching in Biophysical Networks](http://journals.aps.org/prx/abstract/10.1103/PhysRevX.5.031036)\"\n",
    "\n",
    "`7. What other time commitments, such as summer courses, other jobs, planned vacations, etc., will you have over the summer?`\n",
    "\n",
    "I expect to work full time on the project that is 30 or more hours a week.\n",
    "\n",
    "\n",
    "## Contribution to Open-Source Projects\n",
    "* Please find the list of all Pull Requests to Convex.jl repository [here](https://github.com/JuliaOpt/Convex.jl/pulls?q=is%3Apr+author%3AAyush-iitkgp+is%3Aclosed).\n",
    "\n",
    "\n",
    "* Added two-stage method [#6](https://github.com/JuliaDiffEq/DiffEqParamEstim.jl/pull/6)\n",
    "\n",
    "\n",
    "* [Improved DiffEqDocs](https://github.com/JuliaDiffEq/DiffEqDocs.jl/pulls?q=is%3Apr+author%3AAyush-iitkgp+is%3Aclosed).\n",
    "\n",
    "## Experience with Julia\n",
    "I have been using Julia for last one year. In terms of functionality, I like Julia because of its **multiple dispatch** feature as it lets me overload operators with a lot of ease than other programming languages.\n",
    "\n",
    "But the most astonishing feature of Julia is that it is empowering. In other high-level languages, the users can not be developers because developing new packages in those language require the users to know the intricacies of low-level language whereas in Julia, users can develop packages for their needs in Julia itself without compromising with the speed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeline (tentative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Bonding period (May 5 - 30, 2017) \n",
    "\n",
    "My summer vacation will start from 6th of May. During this period, I would want to get myself more familiarized with the other similar projects in languages like MATLAB, python. The idea to take an inspiration from the similar mature projects so that we understand the potential user's requirements, have an idea of the structure of the API we provide to the users so that it is very easy to switch to out implementation once we are ready to get released.\n",
    "\n",
    "I would also like to use this time to undertand the intricacies of Stochastic Differential Equation Models and probabilistic programming language Stan to help me figure out how to provide a support for it in DiffEqParamEstim. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 1\n",
    "**Goal:** *Implement support for more Kernels for two-stage method*\n",
    "\n",
    "I plan to implement the different techniques and algorithms described in the \"Execution\" section on weekly basis. \n",
    "We can merge the code in the main project after each method is implemented. The end outcome of this week would be that the users would be able to provide option for more kernel functions while using two-stage method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 2, 3 and 4\n",
    "**Goal:** *Implement support for different distance measures such as (log-)likelihood and expectation *\n",
    "\n",
    "In this week, I pan to implement the support for log-likelihood cost function which is expected to take 7 days at maximum. Week 3 and 4 would be utilized to implement the second part which is **Expectation maximization algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 5 and 6\n",
    "**Goal:** *Integrate with PenaltyFunctions.jl and implement Tikhonov and Lasso regularization*\n",
    "\n",
    "These 2 weeks would be required to implement regularization techniques such as Tikhonov regularization, LASSO regularization and integrate DiffEqParamEstim with PenaltyFunctions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 7 and 8\n",
    "**Goal:** *Documentation for JuliaCon and attending JuliaCon*\n",
    "\n",
    "By this time, we would have a general set-up for finding the parameters of the differential equations using optimization techniques. If selected for JuliaCon 2017, I would like to take this time to write an easy to understand documentation of the methods implemented so far and travel to Berkeley to present my work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 8 and 9\n",
    "**Goal:** *Provide support for deterministic and heuristic algorithms for global optimum such as BlackBoxOptim, genetic algorithm *\n",
    "\n",
    "During these 2 weeks, I plan to add the support for the 4 packages mentioned in my proposal to find the global optimum for the parameter estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 8\n",
    "**Goal:** *Display the complex domain solution to the user*\n",
    "\n",
    "During this week, I would be writing codes to use the existing machinery in Convex.jl to output the optimum value of the objective function and the value of the primal variables to the users once the optimization is complete.\n",
    "\n",
    "At the end of this week, the support for the complex-domain problems in Convex.jl would be ready for testing for the Julia community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 9 and 10\n",
    "**Goal:** *Documentation, Notebooks, Bug fixes*\n",
    "\n",
    "By this time I will make sure that none of the above implementations has introduced any bug and are complete by documentation as well as testing. I will extend this period to my Future Work as writing example notebooks and\n",
    "preparing for a major release of the package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 11 and 12\n",
    "**Goal:** *Incorporate changes from the feedback given by Julia community and work on presolve routine*\n",
    "\n",
    "During these two weeks, I plan to work on the suggestions given by the Julia community about the new feature I would be adding. I would try to have extensive discussion with the community during this period and incorporate changes so as to make Convex.jl more robust and user-friendly and ready to be released. If time permits, I also plan to understand the existing SDP solvers like SCS and Mosek in detail and try to figure what could be better ways than the existing ones so that we could make our presolve routine better. I would also be required to read and understand the existing presolve routines for Linear Programming Problems so that I could think on similar lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End-Term evaluation\n",
    "**Goal:** *Working towards the publication*\n",
    "\n",
    "Buffer period for any lagging work. I also aim to work towards writing a research paper during this period under the guidance of my mentors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1]. [Parameter Estimation for Differential Equation Models Using a Framework of Measurement Error in Regression Models](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2631937/)\n",
    "\n",
    "[2]. Parameter estimation: the build_loss_objective [#5](https://github.com/JuliaDiffEq/DiffEqParamEstim.jl/issues/5)\n",
    "\n",
    "[3]. [PenaltyFunctions.jl](https://github.com/JuliaML/PenaltyFunctions.jl)\n",
    "\n",
    "[4]. [#103](https://github.com/JuliaOpt/Convex.jl/issues/103) Support for complex variables.\n",
    "\n",
    "[5]. [#191](https://github.com/cvxgrp/cvxpy/issues/191) Add complex variables.\n",
    "\n",
    "[6]. [Chebychev design of an FIR filter given a desired frequency response](http://nbviewer.jupyter.org/github/cvxgrp/cvxpy/blob/master/examples/notebooks/WWW/fir_chebychev_design.ipynb)\n",
    "\n",
    "<!--- ipython nbconvert --to FORMAT notebook.ipynb --->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0-rc3",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
