{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contact\n",
    "\n",
    "**Name**            :   Ayush Pandey\n",
    "\n",
    "**University**      :   [Indian Institute of Technology (IIT), Kharagpur](http://iitkgp.ac.in)\n",
    "\n",
    "**Email**           :   ayushpandey.iitkgp@gmail.com\n",
    "\n",
    "**IRC Handle**      :   KrishnaKanhaiya at freenode.net\n",
    "\n",
    "**Github Username** :   [Ayush-iitkgp](https://github.com/Ayush-iitkgp)\n",
    "\n",
    "**Mentor**          :   [Christopher Rackauckas](https://github.com/ChrisRackauckas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Project\n",
    "\n",
    "## The Problem and Motivations\n",
    "The aim of the project is to add different optimization and bayesian techniques to estimate the parameters of the differential equation in DiffEqParamEstim.jl package and do a comprehensive study of the pros and cons of the different algorithms to add it to the documentation.\n",
    "\n",
    "Differential equation models are widely used in many scientific fields that include engineering, physics, and biomedical sciences. The so-called “forward problem” that is the problem of solving differential equations for given parameter values in the differential equation models has been extensively studied by mathematicians, physicists, and engineers. \n",
    "\n",
    "However, the **inverse problem**, the problem of parameter estimation based on the measurements of output variables, has not been well explored using modern optimization and statistical methods. Parameter estimation aims to find the unknown parameters of the model which give the best fit to a set of experimental data. In this way, parameters which cannot be measured directly will be determined in order to ensure the best fit of the model with the experimental results. This will be done by globally minimizing an objective function which measures the quality of the fit. This inverse problem usually considers a cost function to be optimized (such as maximum likelihood). This problem has applications in systems biology, HIV-AIDS study, and drug dosage estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Plan\n",
    "I propose to implement different algorithms for estimation of the parameters of the differential equations such as:\n",
    "\n",
    "1. Implement support for more Kernels for two-stage method\n",
    "2. Implement support for different distance measures such as (log-)likelihood and expectation maximization algorithm\n",
    "3. Implement regularization techniques such as Tikhonov regularization, LASSO regularization and integrate with PenaltyFunctions.jl\n",
    "4. Provide support for deterministic and heuristic algorithms for global optimum such as BlackBoxOptim, genetic algorithm\n",
    "5. Implement Generalized Profiling Approach using Model Relaxation method for parameter estimation\n",
    "6. Bayesian techniques for parameter estimation using Stan.jl\n",
    "\n",
    "**Stretch Goals**\n",
    "\n",
    "1. Implementing support for parameter estimation for stochastic differential equations   \n",
    "2. Writing native Julia bayesian computational engine using Mamba.jl and Klara.jl\n",
    "\n",
    "\n",
    "I also aim to do a detailed study of the methods and write a proper documentation on where to use the algorithms, precautions to take while using them and the pros and cons of the above methods.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "First of all, we need to understand the mathematical formulation of the inverse parameter estimation problem before we get into the implementation details.\n",
    "\n",
    "### General Form of Parameter Estimation Problem\n",
    "\n",
    "![](images/General_Parameter_Estimation_Form.png)\n",
    "\n",
    "where $x \\in  \\mathbf{R}^{n}$ is the observed state vector, $\\theta \\in {R}^{m}$ is the vector of unknown parameters, f(.) is a known linear or non-linear function vector. In general, we may not observe x directly but we can observe a function of x, here g(.) is the observation functions that maps the state variable to a vector of observable quantities $y \\in {R}^{n}$, these are the signals that can be measured in the experiments.\n",
    "\n",
    "\n",
    "### Maximum likelihood and cost function\n",
    "\n",
    "Maximum likelihood and cost function Assuming that the transformed measurements y are contaminated by additive normally distributed uncorrelated random measurement errors i.e. $y_{ijk}$ = $y_{ijk}(x(t_{i}), θ) + e_{ijk}$ where $e_{ijk}$ ∼ N(0, $σ2_{ijk})$ is the random error with standard deviation $σ_{ijk}$ and $y_{ijk}$ is the measured value, the estimation of the model parameters is formulated as the maximization of the likelihood of the data:\n",
    "\n",
    "![](images/Maximum_Likelihood.png)\n",
    "\n",
    "where $N_{e}$ is the number of experiments, $N_{y,k}$ is the number of observed compounds in the kth experiment, and $N_{t,k,j}$ is the number of measurement time points of the jth observed quantity in the kth experiment.\n",
    "\n",
    "From the theory of maximum likelihood, **the objective is to maximize the likelihood function** and estimate the parameters for which the likelihood function is maximized.\n",
    "\n",
    "Since the likelihood function consists of product terms, we take the log of the likelihood function and negate it to ease our calculation, thus we get log-likelihood function as follows:\n",
    "\n",
    "![](images/Maximum_log_Likelihood.png)\n",
    "\n",
    "**Note** - The maximization of the likelihood function (4) is equivalent to the minimization of the weighted least squares cost function (5).\n",
    "\n",
    "where the residual vector R(·) : $R^{Nθ}$ → $R^{ND}$ is constructed from the squared terms by arranging them to a vector. With this, the model calibration problem can be stated as the well-known nonlinear least-squares (NLS) optimization problem:\n",
    "\n",
    "![](images/NLS.png)\n",
    "\n",
    "A θ vector that solves this optimization problem is called the maximum-likelihood estimates of model parameters.\n",
    "\n",
    "Therefore, the objective of the \"**inverse problem**\" is to pose the problem as the optimization problem and get the optimum value to estimate the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution\n",
    "\n",
    "## 1. Implement support for more Kernels for two-stage method\n",
    "\n",
    "This method is inspired by the research paper [Parameter estimation for Differential Equation Models using a Framework of Measurement Error in Regression Models](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2631937/) where the idea is to estimate the parameters of the Ordinary Differential Equations (ODE) using local smoothing approach and a pseudo-least square.\n",
    "\n",
    "**Note** - This method only works for ordinary differential equations models.\n",
    "\n",
    "Let's rewrite the differential equation written above in another form:\n",
    "\n",
    "![](images/2_stage_form_ODE.png)\n",
    "\n",
    "where X(t) = ${ X1(t), …, Xk(t)}^{T}$ is an unobserved state vector, β = $(β1, …, βm)^{T}$ is a vector of unknown parameters, and F(·) = ${F1(·), …, Fk(·)}^{T}$ is a known linear or nonlinear function vector. In practice, we may not observe X(t) directly, but we can observe its surrogate Y(t). For simplicity, assume an additive measurement error model to relate X(t) to the surrogate Y(t), i.e.,\n",
    "\n",
    "![](images/2_stage_form_Y.png)\n",
    "\n",
    "where the measurement error e(t) is independent of X(t) with a covariance matrix $Σ_{e}$.\n",
    "\n",
    "Suppose X̂′(t) is an estimator of X′(t). Substituting the estimates X̂′(ti), i = 1, …, n, in the ODE equation above, we obtain a regression model:\n",
    "\n",
    "![](images/2_stage_form_reg.png)\n",
    "\n",
    "where Δ($t_{i}$) denotes the substitution error vector, that is Δ($t_{i}$) = X̂′($t_{i}$)− X′($t_{i}$). If X′($t_{i}$) is an unbiased estimator of X′($t_{i}$), Δ($t_{i}$) are errors with mean zero but are not independent. However, if the estimator X′($t_{i}$) is a biased estimator.\n",
    "\n",
    "The idea is to estimate X(ti) and X′(ti) using regression techniques such as local polynomial regression, smoothing spline and the regression spline.\n",
    "\n",
    "I have already [implemented](https://github.com/JuliaDiffEq/DiffEqParamEstim.jl/pull/6), a two-stage method which uses local linear regression to estimate X(t) and local quadratic regression to estimate X′(t). \n",
    "\n",
    "As a consequence, the estimators X̂(t) and X̂′(t) can be expressed as \n",
    "\n",
    "![](images/Parameter_Estimation.png)\n",
    "\n",
    "where \n",
    "\n",
    "![](images/2_stage_form_reg_W.png)\n",
    "\n",
    "and K(.) is a symmetric kernel function $K_{h}$(.) = K(./h)/h and h is a proper bandwidth.\n",
    "\n",
    "Currently, the implementation supports **Epanechnikov**, **Uniform** and **Triangular** kernel functions. I propose to include more kernels such as Quartic, Triweight, Tricube, Gaussian, Cosine, Logistic, Sigmoid function, and Silverman.\n",
    "\n",
    "I also plan to include the advantages and the disadvantages of this method such as :\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "1. Computational efficiency\n",
    "2. Easing of the convergence problem\n",
    "3. The initial values of the state variables of the differential equations not required\n",
    "4. Providing good initial estimates of the unknown parameters for other computationally-intensive methods to further refine the estimates rapidly\n",
    "\n",
    "**Disadvantage**\n",
    "\n",
    "1. This method does not converge to the global/local minima as the Non-Linear regression does in the objective function is convex/concave.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement support for different distance measures such as (log-)likelihood and expectation \n",
    "In the present implementation the \"build_loss_objective\" function only covers the Least-Square objective function and the function defined in [LossFunctions.jl](https://github.com/JuliaML/LossFunctions.jl). It also assumes that the errors in measurement are not correlated i.e\n",
    "\n",
    "![](images/Maximum_Likelihood_no_covariance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the maximum likelihood estimate of x given observations y is the solution to the non-linear least squares problem:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  $x^{*}$ = **min** $||f(x)||^{2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of objective function can easily be constructed using LossFunction.jl. However, there are cases when the error is correlated such as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/Maximum_Likelihood_covariance_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then the maximum likelihood problem to be solved is:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  $x^{*}$ = **min** f'(x)$S^{-1}$f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Thus, I plan to include the functionality to provide the user with te option to state whether the errors are correlated or not and accordingly build the cost/objective function.Also, the present implementation assumes that the variance of the error terms is same which may always be not the case. I will also implement an option for the **weighted least square cost function**.\n",
    "\n",
    "The log-likelihood function reduces to weighted least squares when the error is normally distributed which may not be the case always. Thus, the idea way to go about it would be to provide the option to construct the likelihood function as the cost function. Log-likelihood should also support different versions depending on the type of experimental noise:\n",
    "1. 'homo' homoscedastic noise with constant variance               \n",
    "2. 'homo_var' homoscedastic noise with known non-constant variance\n",
    "3. 'hetero' heteroscedastic noise with variance proportional to the observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement regularization techniques such as Tikhonov regularization, LASSO regularization and integrate with PenaltyFunctions.jl\n",
    "\n",
    "Regularization aims to make the problem less complex (more regular), i.e. to ensure the uniqueness of the solution, to reduce the ill-conditioning and to avoid model overfitting. These techniques are ways to surmount ill-posedness and ill-conditioning.\n",
    "\n",
    "For non-linear cost function, there is no general recipe for the selection of the regularization method. The idea is to provide the users with the options to choose from. [PenaltyFunctions.jl](https://github.com/JuliaML/PenaltyFunctions.jl) provides two main families of penalty functions namely **Element Penalties** and **Array Penalties** which can be used as the regularization function.\n",
    "\n",
    "A comprehensive list of penalty functions provided by PenaltyFunctions.jl are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Element Penalties** \n",
    "\n",
    "\n",
    "Penalty       | value on element\n",
    "--------------|-----------------\n",
    "`NoPenalty()` | `g(θ) = 0`\n",
    "`L1Penalty()` | `g(θ) = abs(θ)`\n",
    "`L2Penalty()` | `g(θ) = 0.5 * θ ^ 2`\n",
    "`ElasticNetPenalty(α = 0.5)` | `g(θ) = (1 - α) * abs(θ) + α * .5 * θ ^ 2`\n",
    "`SCADPenalty(a = 3.7, γ = 1.0)` | `L1Penalty that blends to constant`\n",
    "`MCPPenalty(γ = 2.0)` | `g(θ) = abs(θ) < γ ? abs(θ) - θ ^ 2 / 2γ : γ / 2`\n",
    "`LogPenalty(η = 1.0)` | `g(θ) = log(1 + η * abs(θ))`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Array Penalties**\n",
    "\n",
    "Penalty                | value on array\n",
    "-----------------------|-----------------\n",
    "`NuclearNormPenalty()` | `sum of singular values of x`\n",
    "`MahalanobisPenalty(C)`| `g(x) = x' * C' * C * x`\n",
    "`GroupLassoPenalty()`  | `g(x) = vecnorm(x)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I propose to integrate the PenaltyFunctions.jl with DiffeEqParamEstim to support the different penalty functions as regularization terms in the cost function.\n",
    "\n",
    "There are 2 other penalty type regularization techniques which should be supported, these are:\n",
    "\n",
    "**Tikhonov regularization**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    L($\\theta$) = ($\\theta - \\theta^{ref})^{T} W^{T}W(\\theta - \\theta^{ref})$\n",
    "\n",
    "where W ∈ $R^{N\\theta×N\\theta}$ is a diagonal scaling matrix and $\\theta^{ref}$ ∈ $R^{N\\theta}$ is a reference parameter vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LASSO regularization**\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  ∑$\\theta_{i}$ <= t\n",
    " \n",
    "where t is the value provided by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Provide support for deterministic and heuristic algorithms for global optimum such as BlackBoxOptim, genetic algorithm\n",
    "\n",
    "It is well-known that the cost function (5) can be highly nonlinear and nonconvex in the model parameters. Many efficient local optimization algorithms have been developed to find the solution of nonlinear least squares problems, including Gauss-Newton, Levenberg-Marquardt and trust-region methods. These local methods are especially efficient when provided with high quality first (gradient, Jacobian) and second order (Hessian) information via parametric sensitivities. However, in this type of problems they will likely converge to local solutions close to the initial guess of the parameters. Thus, there is a need to integrate global optimization algorithm packages with DiffEqParamEstim. [JuliaOpt]() currently supports the following global optimization algorithms:\n",
    "\n",
    "1. [BlackBoxOptim](https://github.com/robertfeldt/BlackBoxOptim.jl) - The present cost function supports optimization using BlackBoXOptim. I will add an example to illustrate the users on how to use it.\n",
    "2. [Evolutionary](https://github.com/wildart/Evolutionary.jl)\n",
    "3. [GeneticAlgorithms](https://github.com/WestleyArgentum/GeneticAlgorithms.jl)\n",
    "4. [StochasticSearch](https://github.com/phrb/StochasticSearch.jl) such as Tabu Search and Simulated Annealing\n",
    "\n",
    "The idea is to mould the cost function (least square or likelihood) generated during parameter estimation to be in the format accepted by the above solvers.\n",
    "\n",
    "**Note** - Deterministic global optimization methods can guarantee global optimality but their computationally cost increases exponentially with the number of estimated parameters. Alternatively, stochastic and heuristic methods can be used as more practical alternatives, usually obtaining adequate solutions in reasonable computation times, although at the price of no guarantees. In such context, metaheuristics, hybrids (i.e. combinations) with efficient local search methods have been particularly successful, I would also like to look at an approach to combine the deterministic and the heuristic techniques to converge to the exact global optimum in a reasonable time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Implement Generalized Profiling Approach using Model Relaxation method for parameter estimation\n",
    "\n",
    "The likelihood, least squares and the two-stage methods to estimate the parameters of the differential equations from noisy data are computationally intensive and are often poorly suited to statistical techniques such as inference and interval estimation. The generalized smoothing approach is is based on the modification of data smoothing methods along with a generalization of profiled estimation and have been successfully deployed to find the interval estimates of the parameter.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "Given the differential equation of the form:\n",
    "\n",
    "![](images/smoothing_1.png)\n",
    "\n",
    "The idea is to estimate the solution $\\hat{x(t)}$ as the linear combination of the basis functions $\\phi_{i}$(t) as:\n",
    "\n",
    "![](images/smoothing_2.png)\n",
    "\n",
    "where the number $K_{i}$ of basis functions in vector $\\phi_{i}$ is chosen so as to ensure enough exibility to capture the variation in xi and its derivatives that is required to satisfy the above differential equation. \n",
    "\n",
    "**Note** - Although the original collocation methods used polynomial bases, the choice of $\\phi_{i}$(t) is taken to be spline because of it's computational efficiency, also because it allows control over the smoothness of the solution at specic values of t.\n",
    "\n",
    "**The first step is to find $c'_{i}$** in terms of the parameter $\\theta$ and the smoothing parameter $\\lambda$ using model fitting and data fitting simultaneously as:\n",
    "\n",
    "![](images/smoothing_3.png)\n",
    "\n",
    "where PEN(x|$\\lambda$) is given by \n",
    "\n",
    "![](images/smoothing_4.png)\n",
    "\n",
    "Once, we have the $c'_{i}$, the next step is to find the estimates of the parameter $\\theta$ using data fitting as:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  **Min** $\\sum(y(t_{i})-\\hat{x(t_{i})})^{2}$\n",
    "\n",
    "**Note** - On the computational side, this method is as fast or faster than NLS and other approaches, and much faster than the Bayesian-MCMC method, which has comparable estimation efficiency. Unlike MCMC, the generalized profiling approach is relatively straightforward to deploy to a wide range of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bayesian techniques for parameter estimation using Stan.jl\n",
    "\n",
    "Stan is an open-source probabilistic programming language that performs Bayesian inference for user-specifed models. [Stan.jl](https://github.com/goedman/Stan.jl) is the Julia interface to the stan library.\n",
    "\n",
    "Stan provides a built-in mechanism for specifying and solving systems of ordinary differential equations. Stan provides two different integrators, one tuned for solving non-stiff systems and one for stiff systems.\n",
    "\n",
    "* rk45: a fourth and fifth order Runge-Kutta method for non-stiff systems \n",
    "* bdf: a variable-step, variable-order, backward-differentiation formula implementation for stiff systems\n",
    "\n",
    "**Note** - The stiff solvers are slower, but more robust.\n",
    "\n",
    "Now, consider the ODE:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   dy1 = y2           &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;       dy2 = -y1 - $\\theta$*y2\n",
    "\n",
    "The above ODE can be translated into DifferentialEquations format as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using DifferentialEquations\n",
    "f = @ode_def Harmonic begin\n",
    "  dy1 = y2\n",
    "  dy2 = -y1 - a*y2\n",
    "end a=>1.5\n",
    "\n",
    "u0 = [1.0;1.0]\n",
    "tspan = (0.0,10.0)\n",
    "prob = ODEProblem(f,u0,tspan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above ODE is also translated into Stan program given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data {\n",
    "    int<lower=1> T;\n",
    "    vector[2] y0;\n",
    "    real ts[T];\n",
    "    real theta[1];\n",
    "    }\n",
    "model {\n",
    "    }\n",
    "generated quantities {\n",
    "    vector[2] y_hat[T];\n",
    "    matrix[2, 2] A;\n",
    "    A[1, 1] = 0;\n",
    "    A[1, 2] = 1;\n",
    "    A[2, 1] = -1;\n",
    "    A[2, 2] = -theta[1];\n",
    "    for (t in 1:T)\n",
    "    y_hat[t] = matrix_exp((t - 1) * A) * y0;\n",
    "    // add measurement error\n",
    "    for (t in 1:T) {\n",
    "        y_hat[t, 1] = y_hat[t, 1] + normal_rng(0, 0.1);\n",
    "        y_hat[t, 2] = y_hat[t, 2] + normal_rng(0, 0.1);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our objective is to translate the ODE described in DifferentialEquations.jl using ParameterizedFunctions.jl into the corresponding Stan above and use Stan.jl to find the Bayesian estimates of the parameters.**\n",
    "\n",
    "Fortunately, ParametrizedFunctions.jl already provides us the option to extract the information the ODE model such as:\n",
    "\n",
    "<!-- **Note- ** The **\"evaluate\"** method in the existing atoms can handle the complex variables. But some atoms are only defined over real domain (for example- max(x,0)) thus, I would need to append the warning messages to such atoms in case the users by mistake call the evaluate method with complex argument. --->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin  # In[6], line 3:\n",
      "    dy1 = y2 # In[6], line 4:\n",
      "    dy2 = -y1 - a * y2\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "println(f.origex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbol[:a]\n",
      "Expr[quote \n",
      "    du[1] = 1 * u[2]\n",
      "    du[2] = -(u[1]) - a * u[2]\n",
      "    nothing\n",
      "end]\n"
     ]
    }
   ],
   "source": [
    "println(f.params) # prints the parameter of the differential equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize the above process in 3 steps:\n",
    "1. Express the ODE model in Julia using ```ParametrizedFunctions.jl```.\n",
    "2. Convert the ParametrizedFunctions ODE model to Stan model using the functionalities provided by ParametrizedFunctions \n",
    "3. Use Stan.jl to generate posterior distribution of parameters and return it to the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Me\n",
    "\n",
    "## Personal Background\n",
    "\n",
    "I am a final year graduate student pursuing an Integrated Master of Science(MS) degree in Mathematics and Computing Sciences with specialization in Optimization at IIT Kharagpur, India. I am proficient in C, C++, Python, and Julia.\n",
    "\n",
    "## Previous Relevant Experience\n",
    "\n",
    "I am a former Google Summer of the Code student with the Julia Language where I worked on the project titled **Support for complex numbers within Convex.jl**. As a result of our work, we became the first open-source DCP package to support optimization with complex-variables. I have also worked on different projects related to optimization and machine learning. Please find all my projects [here](https://drive.google.com/drive/folders/0B2oOdWdSJWa1RWRsQWdCZ25LUXc). I have taken courses on differential equations such as Partial Differential Equations, Numerical solution of ODE and PDE and Advanced NUmnerical Techniques(Theory and Lab) where I have written MATLAB/C [code](https://github.com/Ayush-iitkgp/Numerical-Solution-of-ODE-and-PDE) for different numerical techniques used in solving ODEs and PDEs. \n",
    "\n",
    "## Relevant Courses\n",
    "* Partial Differential Equations\n",
    "* Numerical solution of ODE and PDE\n",
    "* Advanced Numerical Techniques\n",
    "* Stochastic Processes\n",
    "* Non-Linear Programming\n",
    "* Convex Optimization\n",
    "* Probability & Statistics, Regression, Generalized Linear Models\n",
    "* Linear Algebra, Programming and Data Structures, Object Oriented System Design\n",
    "\n",
    "\n",
    "## Answers of listed questions\n",
    "\n",
    "`1. What do you want to have completed by the end of the program?`\n",
    "\n",
    "By the end of the program, I want to support different optimization and bayesian techniques used to estimate the parameters of the differential equations given the experimental data. Presently, DiffEqParamEstim only supports Non-Linear Regression technique to estimate the parameters. I aim to implement numerous techniques such as two-stage method, model relaxation technique, regularization, log-likelihood estimation, meta-heuristic techniques for global optimum and bayesian techniques using Stan.jl.\n",
    "\n",
    "`2. Who’s interested in the work, and how will it benefit them?`\n",
    "\n",
    "The problem of parameter estimation has extensive use in the following fields:\n",
    "1. HIV-AIDS viral dynamics\n",
    "2. Systems biology\n",
    "3. Drug dosage estimation\n",
    "\n",
    "`3. What are the potential hurdles you might encounter, and how can you resolve them?`\n",
    "\n",
    "In order to link Stan with DiffEqParamEstim, I will have to understand how Stan works. I will also need to go through the theory of Markov Chain Monte Carlo and Hamiltonian Monte Carlo to write in-situ Bayesian estimator for parameters of the diffeenetial equations. Lastly, I would need to understand the theoty behind the stochastic differential equations to write an implementation to find it's parameters. \n",
    "\n",
    "`4. How will you prioritize different aspects of the project like features, API usability, documentation and robustness?`\n",
    "\n",
    "Please refer to the \"Timeline\" section where I have described in details about my plans to tackle different aspects of the project.\n",
    "\n",
    "`5. Does your project have any milestones that you can target throughout the period?`\n",
    "\n",
    "Yes, before JuliaCon 2017, I would have implemented the various optimization and regualarization algorithms to estimate the parameters and would be ready for testing for the Julia community.\n",
    "\n",
    "`6. Are there any stretch goals you can make if the main project goes smoothly?`\n",
    "\n",
    "Yes, I would try to implement native julia computational engine for Bayesian estimation using tools like Mamba.jl or Klara.jl and the inverse problem for Stochastic Differential Equations based on the paper \"[Control of Stochastic and Induced Switching in Biophysical Networks](http://journals.aps.org/prx/abstract/10.1103/PhysRevX.5.031036)\"\n",
    "\n",
    "`7. What other time commitments, such as summer courses, other jobs, planned vacations, etc., will you have over the summer?`\n",
    "\n",
    "I expect to work full time on the project that is 30 or more hours a week.\n",
    "\n",
    "\n",
    "## Contribution to Open-Source Projects\n",
    "* Please find the list of all Pull Requests to Convex.jl repository [here](https://github.com/JuliaOpt/Convex.jl/pulls?q=is:pr+author:Ayush-iitkgp+is:closed).\n",
    "\n",
    "\n",
    "* Added two-stage method [#6](https://github.com/JuliaDiffEq/DiffEqParamEstim.jl/pull/6)\n",
    "\n",
    "\n",
    "* [Improved DiffEqDocs](https://github.com/JuliaDiffEq/DiffEqDocs.jl/pulls?q=is:pr+author:Ayush-iitkgp+is:closed).\n",
    "\n",
    "## Experience with Julia\n",
    "I have been using Julia for last one year. In terms of functionality, I like Julia because of its **multiple dispatch** feature as it lets me overload operators with a lot of ease than other programming languages.\n",
    "\n",
    "But the most astonishing feature of Julia is that it is empowering. In other high-level languages, the users can not be developers because developing new packages in those language require the users to know the intricacies of low-level language whereas in Julia, users can develop packages for their needs in Julia itself without compromising with the speed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeline (tentative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Bonding period (May 5 - 30, 2017) \n",
    "\n",
    "My summer vacation will start from 6th of May. During this period, I would want to get myself more familiarized with the other similar projects in languages like MATLAB, python. The idea to take an inspiration from the similar mature projects so that we understand the potential user's requirements, have an idea of the structure of the API we provide to the users so that it is very easy to switch to out implementation once we are ready to get released.\n",
    "\n",
    "I would also like to use this time to understand the intricacies of Stochastic Differential Equation Models and probabilistic programming language Stan to help me figure out how to provide a support for it in DiffEqParamEstim. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 1\n",
    "**Goal:** *Implement support for more Kernels for two-stage method*\n",
    "\n",
    "I plan to implement the different techniques and algorithms described in the \"Execution\" section on weekly basis. \n",
    "We can merge the code in the main project after each method is implemented. The end outcome of this week would be that the users would be able to provide an option for more kernel functions while using the two-stage method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 2, 3 and 4\n",
    "**Goal:** *Implement support for different distance measures such as (log-)likelihood and expectation *\n",
    "\n",
    "In this week, I plan to implement the support for log-likelihood cost function which is expected to take 1 week at maximum. Week 3 and 4 would be utilized to implement the second part which is **Expectation maximization algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 5 and 6\n",
    "**Goal:** *Integrate with PenaltyFunctions.jl and implement Tikhonov and Lasso regularization*\n",
    "\n",
    "These 2 weeks would be required to implement regularization techniques such as Tikhonov regularization, LASSO regularization and integrate DiffEqParamEstim with PenaltyFunctions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 7 and 8\n",
    "**Goal:** *Documentation for JuliaCon and attending JuliaCon*\n",
    "\n",
    "By this time, we would have a general set-up for finding the parameters of the differential equations using optimization techniques. If selected for JuliaCon 2017, I would like to take this time to write an easy to understand documentation of the methods implemented so far and travel to Berkeley to present my work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 8\n",
    "**Goal:** *Provide support for deterministic and heuristic algorithms for global optimum such as BlackBoxOptim, genetic algorithm *\n",
    "\n",
    "During this week, I plan to add the support for the 4 packages mentioned in my proposal to find the global optimum for the parameter estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 9 and 10\n",
    "**Goal:** *Implement Generalized Profiling Approach using Model Relaxation method for parameter estimation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 11 and 12\n",
    "**Goal:** *Link with Stan.jl to estimate the parameters using Bayesian inference*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End-Term evaluation\n",
    "**Goal:** *Working to implement the strech goals*\n",
    "\n",
    "Buffer period for any lagging work. I also aim to work towards implementing the stretch goals such as to implementing support for parameter estimation for stochastic differential equations and write native Julia bayesian computational engine using Mamba.jl and Klara.jl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1]. [Parameter Estimation for Differential Equation Models Using a Framework of Measurement Error in Regression Models](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2631937/)\n",
    "\n",
    "[2]. Parameter estimation: the build_loss_objective [#5](https://github.com/JuliaDiffEq/DiffEqParamEstim.jl/issues/5)\n",
    "\n",
    "[3]. [PenaltyFunctions.jl](https://github.com/JuliaML/PenaltyFunctions.jl)\n",
    "\n",
    "[4]. [Robust and efficient parameter estimation in dynamic models of biological systems](http://bmcsystbiol.biomedcentral.com/articles/10.1186/s12918-015-0219-2)\n",
    "\n",
    "[5]. [Parameter Estimation for Differential Equations: A Generalized Smoothing Approach](http://faculty.bscb.cornell.edu/~hooker/ODE_Estimation.pdf)\n",
    "\n",
    "[6]. [Stan: A probabilistic programming language for Bayesian inference and optimization](http://www.stat.columbia.edu/~gelman/research/published/stan_jebs_2.pdf)\n",
    "\n",
    "[7]. Linking with Stan project [#135](https://github.com/JuliaDiffEq/DifferentialEquations.jl/issues/135)\n",
    "\n",
    "<!--- ipython nbconvert --to FORMAT notebook.ipynb --->"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0-rc3",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
